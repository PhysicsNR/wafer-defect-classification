{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4000137d",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸŸ¢ Wafer Defect Classification (WM-811K)\n",
    "\n",
    "This project applies **deep learning** to classify wafer maps from the **WM-811K dataset** into nine defect categories using **ResNet18 (transfer learning)**. \n",
    "\n",
    "It demonstrates:\n",
    "- Data preparation (parsing waferMap â†’ images)\n",
    "- Training with transfer learning\n",
    "- Evaluation with precision/recall/F1, ROC, PR curves\n",
    "- Interpretability using **Grad-CAM**\n",
    "- Insights into imbalance challenges in semiconductor yield analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfee6952",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ“Š Dataset: WM-811K\n",
    "\n",
    "- **Size:** 811,457 wafer maps\n",
    "- **Labels:** ~172,950 wafers labeled into 9 defect classes  \n",
    "  *center, donut, edge-loc, edge-ring, loc, near-full, random, scratch, none*  \n",
    "- **Imbalance:** Majority class (\"none\") = 147k, rare class (\"near-full\") = 149  \n",
    "- Each wafer is stored as a 2D matrix (`waferMap`):  \n",
    "  - 0 = background  \n",
    "  - 1 = normal die  \n",
    "  - 2 = defective die  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7abc3de",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ§® Mathematical Foundations\n",
    "\n",
    "### Softmax + Cross-Entropy Loss\n",
    "$$\n",
    "\\hat{p}_k = \\frac{\\exp(z_k)}{\\sum_j \\exp(z_j)}\n",
    "$$\n",
    "$$\n",
    "\\mathcal{L}_{CE} = - \\frac{1}{N} \\sum_{i=1}^N \\log \\hat{p}_{y_i}\n",
    "$$\n",
    "\n",
    "### Precision, Recall, and F1\n",
    "$$\n",
    "Precision = \\frac{TP}{TP + FP}, \\quad Recall = \\frac{TP}{TP + FN}, \\quad\n",
    "F1 = \\frac{2 \\cdot Precision \\cdot Recall}{Precision + Recall}\n",
    "$$\n",
    "\n",
    "### ROC & PR Curves\n",
    "- **ROC**: plots TPR vs FPR  \n",
    "$$TPR = \\frac{TP}{TP+FN}, \\quad FPR = \\frac{FP}{FP+TN}$$  \n",
    "\n",
    "- **PR**: plots Precision vs Recall (important for imbalance)\n",
    "\n",
    "### Convolutional Layers\n",
    "$$\n",
    "y_{i,j,k} = \\sigma\\!\\Bigg(\\sum_{m=1}^{M}\\sum_{u}\\sum_{v} W_{u,v,m,k}\\; x_{i+u, j+v, m} + b_k\\Bigg)\n",
    "$$\n",
    "\n",
    "### Residual Block (ResNet18)\n",
    "$$\n",
    "y = F(x, \\{W_i\\}) + x\n",
    "$$\n",
    "\n",
    "- $F(x)$ = learned residual mapping  \n",
    "- $x$ = identity skip connection  \n",
    "- Benefit: mitigates vanishing gradients and allows deeper networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5228dfd5",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ’» Data Preparation\n",
    "Wafer maps are parsed into 224Ã—224 RGB images grouped into `train/val/test` directories by class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bb3ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def render_wafer(arr):\n",
    "    arr = np.array(arr).astype(np.uint8)\n",
    "    vis = (arr==1)*180 + (arr==2)*255\n",
    "    return Image.fromarray(vis, mode='L').resize((224,224)).convert('RGB')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5f5b16",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ§  Model: ResNet18 (Transfer Learning)\n",
    "\n",
    "We use **ResNet18 pretrained on ImageNet**, replacing the final FC layer with 9 output nodes.\n",
    "\n",
    "Training objective: **Cross-Entropy Loss** with class weights for imbalance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2417725e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch, torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "def build_model(num_classes=9, pretrained=True, freeze_backbone=True):\n",
    "    weights = models.ResNet18_Weights.DEFAULT if pretrained else None\n",
    "    m = models.resnet18(weights=weights)\n",
    "    if freeze_backbone:\n",
    "        for name, param in m.named_parameters():\n",
    "            if not name.startswith('fc.'):\n",
    "                param.requires_grad = False\n",
    "    in_feats = m.fc.in_features\n",
    "    m.fc = nn.Linear(in_feats, num_classes)\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7441c9",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ”„ Training Loop\n",
    "\n",
    "Parameters $\\theta$ optimized with Adam:\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\eta \\cdot \\nabla_\\theta \\mathcal{L}(\\theta)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e96a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device='cuda'):\n",
    "    model.train(); total, correct, loss_sum = 0,0,0\n",
    "    for xb,yb in loader:\n",
    "        xb,yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits,yb)\n",
    "        loss.backward(); optimizer.step()\n",
    "        loss_sum += loss.item()*xb.size(0)\n",
    "        correct += (logits.argmax(1)==yb).sum().item()\n",
    "        total += xb.size(0)\n",
    "    return loss_sum/total, correct/total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d36df06",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ“ˆ Results\n",
    "\n",
    "### Training Dynamics\n",
    "![Train Loss](models/train_loss.png)  \n",
    "![Train Acc](models/train_acc.png)\n",
    "\n",
    "### Classification Report\n",
    "![Classification Report](models/classification_report.png)\n",
    "\n",
    "### Confusion Matrix\n",
    "![Confusion Matrix](models/confusion_matrix_norm.png)\n",
    "\n",
    "### ROC & PR Curves\n",
    "![ROC](models/ovr_roc.png)  \n",
    "![PR](models/ovr_pr.png)\n",
    "\n",
    "### Grad-CAM\n",
    "![Grad-CAM Example](models/gradcam/example_cam.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67503579",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ”Ž Discussion\n",
    "\n",
    "- **Strong baseline**: ResNet18 pretrained reached ~93% overall accuracy.  \n",
    "- **Imbalance issue**: Macro-F1 is lower due to rare classes.  \n",
    "- **Interpretability**: Grad-CAM overlays confirm the model focuses on defect regions.  \n",
    "- **Industry relevance**: Mirrors real-world fab challenges.\n",
    "\n",
    "## ðŸš€ Next Steps\n",
    "- Use focal loss or oversampling for imbalance  \n",
    "- Try deeper backbones  \n",
    "- Add K-fold cross-validation  \n",
    "- Deploy via Streamlit or SageMaker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b817106e",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ“Œ References\n",
    "- He et al., 2016: *Deep Residual Learning for Image Recognition*  \n",
    "- WM-811K Dataset (Kaggle)  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
